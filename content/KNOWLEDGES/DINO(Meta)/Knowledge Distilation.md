# Knowledge Distilation
---
## What is 'Knowledge Distilation'?

Soft assignment 방식의 일종

**작은 model**이, 더 **큰 model**의 output을 mimic하도록 학습(train)하는 것
-> model compression

## How?

#### Teacher network

#### Student network